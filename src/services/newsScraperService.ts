// News Scraper Service for Korean News Sources (FALLBACK ONLY)
// This service is now deprecated in favor of backend API
// Backend now handles real scraping from Korean news sources
import { NewsArticle } from './newsApiService';

export interface NewsSource {
  id: string;
  name: string;
  baseUrl: string;
  icon: string;
  scrapeMethod: 'api' | 'rss' | 'web';
}

export const koreanNewsSources: NewsSource[] = [
  {
    id: 'SBS',
    name: 'SBS',
    baseUrl: 'https://news.sbs.co.kr',
    icon: 'ğŸ“º',
    scrapeMethod: 'rss'
  },
  {
    id: 'ì—°í•©ë‰´ìŠ¤',
    name: 'ì—°í•©ë‰´ìŠ¤',
    baseUrl: 'https://www.yna.co.kr',
    icon: 'ğŸ—ï¸',
    scrapeMethod: 'rss'
  },
  {
    id: 'KBS',
    name: 'KBS',
    baseUrl: 'https://news.kbs.co.kr',
    icon: 'ğŸ“º',
    scrapeMethod: 'rss'
  },
  {
    id: 'ì¤‘ì•™ì¼ë³´',
    name: 'ì¤‘ì•™ì¼ë³´',
    baseUrl: 'https://www.donga.com',
    icon: 'ğŸ“°',
    scrapeMethod: 'rss'
  }
];

class NewsScraperService {
  // Mock data generator for each source
  private generateMockNewsForSource(sourceId: string, count: number = 10): NewsArticle[] {
    const sourceInfo = koreanNewsSources.find(s => s.id === sourceId);
    if (!sourceInfo) return [];

    const mockTitles = {
      'SBS': [
        'SBS 8ë‰´ìŠ¤: í•œêµ­ ê²½ì œ ì„±ì¥ë¥  ì „ë§ ìƒí–¥ ì¡°ì •',
        'SBS íŠ¹ì§‘: K-pop ì•„ì´ëŒì˜ ê¸€ë¡œë²Œ ì˜í–¥ë ¥ ë¶„ì„',
        'SBS ë¦¬í¬íŠ¸: ì„œìš¸ ì§€í•˜ì²  ë…¸ì„  í™•ì¥ ê³„íš ë°œí‘œ',
        'SBS ë‚ ì”¨ì •ë³´: ì´ë²ˆ ì£¼ ì „êµ­ ê¸°ì˜¨ ë³€í™” ì˜ˆìƒ',
        'SBS ìŠ¤í¬ì¸ ë‰´ìŠ¤: í•œêµ­ ì¶•êµ¬ëŒ€í‘œíŒ€ ìµœì‹  ì†Œì‹',
        'SBS ë¬¸í™”: í•œë³µ ë””ìì¸ í˜ì‹ ìœ¼ë¡œ ì Šì€ì¸µ ê´€ì‹¬ ì§‘ì¤‘',
        'SBS ì‚¬íšŒ: ë””ì§€í„¸ êµìœ¡ í”Œë«í¼ í™•ì‚° í˜„í™©'
      ],
      'ì—°í•©ë‰´ìŠ¤': [
        'ì—°í•©ë‰´ìŠ¤ ì†ë³´: í•œë¯¸ ì •ìƒíšŒë‹´ ì£¼ìš” ì„±ê³¼ ë¶„ì„',
        'ì—°í•©ë‰´ìŠ¤: êµ­ì •ê°ì‚¬ ì£¼ìš” ì´ìŠˆ ë° ìŸì  ì •ë¦¬',
        'ì—°í•©ë‰´ìŠ¤ ê²½ì œ: ë°˜ë„ì²´ ì‚°ì—… ê¸€ë¡œë²Œ ë™í–¥ ë¶„ì„',
        'ì—°í•©ë‰´ìŠ¤: ì½”ìŠ¤í”¼ ì§€ìˆ˜ ìƒìŠ¹ì„¸ ì§€ì†, íˆ¬ì ì „ë§',
        'ì—°í•©ë‰´ìŠ¤ ì‚¬íšŒ: ì „êµ­ ëŒ€í•™ ì…ì‹œ ë™í–¥ ë° ë³€í™”',
        'ì—°í•©ë‰´ìŠ¤: CBDC ë„ì… ì—°êµ¬ê°œë°œ í˜„í™©',
        'ì—°í•©ë‰´ìŠ¤ êµ­ì œ: í•œêµ­ ì™¸êµì •ì±… ìƒˆë¡œìš´ ë°©í–¥'
      ],
      'KBS': [
        'KBS ë‰´ìŠ¤9: ì •ë¶€ ì •ì±… ë³€í™” ë° í–¥í›„ ê³„íš',
        'KBS ë¬¸í™”ë‰´ìŠ¤: í•œêµ­ ì „í†µë¬¸í™” ë³´ì¡´ ë…¸ë ¥ê³¼ ì„±ê³¼',
        'KBS êµìœ¡ë‰´ìŠ¤: ë””ì§€í„¸ êµìœ¡ í˜ì‹  ë°©ì•ˆ ë°œí‘œ',
        'KBS ê±´ê°•ë‰´ìŠ¤: ê²¨ìš¸ì²  ê±´ê°• ê´€ë¦¬ ì „ë¬¸ê°€ ì¡°ì–¸',
        'KBS ê³¼í•™ë‰´ìŠ¤: ëˆ„ë¦¬í˜¸ ì„±ê³µê³¼ ìš°ì£¼ ê¸°ìˆ  ë°œì „',
        'KBS ìŠ¤í¬ì¸ : ì†í¥ë¯¼ í† íŠ¸ë„˜ 200ê³¨ ë‹¬ì„± ì†Œì‹',
        'KBS ì—°ì˜ˆ: K-pop ì‚°ì—… ê¸€ë¡œë²Œ ì„±ì¥ì„¸ ì§€ì†'
      ],
      'ì¤‘ì•™ì¼ë³´': [
        'ì¤‘ì•™ì¼ë³´ ì‚¬ì„¤: í•œêµ­ ì‚¬íšŒ ë³€í™” ë°©í–¥ì„±ê³¼ ê³¼ì œ',
        'ì¤‘ì•™ì¼ë³´ ê²½ì œ: ê¸€ë¡œë²Œ ê²½ì œ ìœ„ê¸° ëŒ€ì‘ ì „ëµ',
        'ì¤‘ì•™ì¼ë³´ ë¬¸í™”: í•œêµ­ ì˜í™” í•´ì™¸ ì§„ì¶œ ì„±ê³¼',
        'ì¤‘ì•™ì¼ë³´ ì •ì¹˜: êµ­íšŒ ì£¼ìš” ë²•ì•ˆ ì²˜ë¦¬ í˜„í™©',
        'ì¤‘ì•™ì¼ë³´ ì‚¬íšŒ: ì¸êµ¬ ë³€í™”ì™€ ì‚¬íšŒ ì •ì±… ëŒ€ì‘',
        'ì¤‘ì•™ì¼ë³´ IT: AI ë°˜ë„ì²´ ì‹œì¥ ì§„ì¶œ ê°€ì†í™”',
        'ì¤‘ì•™ì¼ë³´ ê³¼í•™: ìš°ì£¼í•­ê³µ ê¸°ìˆ  ë°œì „ê³¼ ì „ë§'
      ]
    };

    const categories = ['politics', 'economy', 'society', 'culture', 'sports', 'entertainment', 'technology'];
    const difficulties: Array<'beginner' | 'intermediate' | 'advanced'> = ['beginner', 'intermediate', 'advanced'];
    
    const titles = mockTitles[sourceId as keyof typeof mockTitles] || mockTitles['ì—°í•©ë‰´ìŠ¤'];

    return Array.from({ length: count }, (_, index) => {
      const titleIndex = index % titles.length;
      const category = categories[Math.floor(Math.random() * categories.length)];
      const difficulty = difficulties[Math.floor(Math.random() * difficulties.length)];
      
      return {
        id: `${sourceId.toLowerCase()}-${Date.now()}-${index}`,
        title: titles[titleIndex],
        subtitle: `${sourceInfo.name}ì—ì„œ ì œê³µí•˜ëŠ” ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¶„ì„í•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”.`,
        summary: `${sourceInfo.name}ì˜ ì£¼ìš” ë‰´ìŠ¤ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.`,
        content: `${titles[titleIndex]}ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤. ${sourceInfo.name}ì—ì„œ ì‹¬ì¸µ ì·¨ì¬í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì´ìŠˆëŠ” í•œêµ­ ì‚¬íšŒì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.`,
        source: sourceInfo.name,
        author: `${sourceInfo.name} ê¸°ì`,
        category: category,
        difficulty: difficulty,
        readingTime: Math.floor(Math.random() * 10) + 3,
        imageUrl: `https://images.unsplash.com/photo-${1500000000000 + Math.floor(Math.random() * 100000000)}?w=800&h=400&fit=crop`,
        keywords: ['í•œêµ­', 'ë‰´ìŠ¤', sourceInfo.name],
        tags: [category, sourceInfo.name, difficulty],
        publishedDate: new Date(Date.now() - Math.random() * 24 * 60 * 60 * 1000).toISOString(),
        views: Math.floor(Math.random() * 5000) + 100,
        likes: Math.floor(Math.random() * 200) + 10,
        bookmarks: Math.floor(Math.random() * 100) + 5
      };
    });
  }

  // Get news from specific source
  async getNewsFromSource(sourceId: string, limit: number = 20): Promise<NewsArticle[]> {
    try {
      // In a real implementation, this would scrape from the actual website
      // For now, we'll return mock data
      console.log(`Fetching news from ${sourceId}`);
      
      // Simulate API delay
      await new Promise(resolve => setTimeout(resolve, 1000));
      
      return this.generateMockNewsForSource(sourceId, limit);
    } catch (error) {
      console.error(`Error fetching news from ${sourceId}:`, error);
      return this.generateMockNewsForSource(sourceId, limit);
    }
  }

  // Get aggregated news from all sources
  async getAggregatedNews(limit: number = 20): Promise<NewsArticle[]> {
    try {
      const allNews: NewsArticle[] = [];
      
      // Get news from each source
      for (const source of koreanNewsSources) {
        const sourceNews = await this.getNewsFromSource(source.id, Math.ceil(limit / koreanNewsSources.length));
        allNews.push(...sourceNews);
      }
      
      // Sort by published date (newest first) and limit results
      return allNews
        .sort((a, b) => new Date(b.publishedDate).getTime() - new Date(a.publishedDate).getTime())
        .slice(0, limit);
    } catch (error) {
      console.error('Error getting aggregated news:', error);
      return [];
    }
  }

  // Get news by category from all sources
  async getNewsByCategory(category: string, limit: number = 20): Promise<NewsArticle[]> {
    try {
      const allNews = await this.getAggregatedNews(limit * 2); // Get more to filter by category
      
      return allNews
        .filter(news => news.category === category)
        .slice(0, limit);
    } catch (error) {
      console.error(`Error getting news by category ${category}:`, error);
      return [];
    }
  }

  // Search news across all sources
  async searchNews(query: string, limit: number = 20): Promise<NewsArticle[]> {
    try {
      const allNews = await this.getAggregatedNews(limit * 2);
      
      const searchLower = query.toLowerCase();
      return allNews
        .filter(news => 
          news.title.toLowerCase().includes(searchLower) ||
          news.subtitle.toLowerCase().includes(searchLower) ||
          news.keywords.some(keyword => keyword.toLowerCase().includes(searchLower))
        )
        .slice(0, limit);
    } catch (error) {
      console.error(`Error searching news with query ${query}:`, error);
      return [];
    }
  }
}

export const newsScraperService = new NewsScraperService();

// Helper functions for web scraping (future implementation)
export const webScrapingHelpers = {
  // Extract title from HTML
  extractTitle: (html: string, selector: string): string => {
    // This would use a proper HTML parser in real implementation
    return '';
  },

  // Extract content from HTML
  extractContent: (html: string, selector: string): string => {
    // This would use a proper HTML parser in real implementation
    return '';
  },

  // Extract image URL from HTML
  extractImageUrl: (html: string, selector: string): string => {
    // This would use a proper HTML parser in real implementation
    return '';
  },

  // Clean and format text
  cleanText: (text: string): string => {
    return text.trim().replace(/\s+/g, ' ');
  }
};

export default newsScraperService;